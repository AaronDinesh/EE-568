{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a05c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do the imports -- no need to change this\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import sys\n",
    "sys.path.insert(0, \"src/\")\n",
    "from environment import GridWorldEnvironment\n",
    "from MDPsolver import MDPsolver\n",
    "from utils import *\n",
    "from plot import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2665f",
   "metadata": {},
   "source": [
    "***Before starting, we recall the use of the gridworld environment.***\n",
    "\n",
    "The gridworld environment is instantiated via the class `GridWorldEnvironment`. \n",
    "\n",
    "***It takes 4 input values:***\n",
    "- `reward_mode` : Integer between 0 and 3 for different reward profiles,\n",
    "- `size`: Gridworld size,\n",
    "- `prop`: Probability assigned to the event that the agent does not follow the chosen action but another one selected uniformely at random,\n",
    "- `gamma`: Discount factor of the environment.\n",
    "\n",
    "***Interface of a Gridworld instance:***\n",
    "- `print(gridworld.n_states)` # return the number of states\n",
    "- `print(gridworld.n_actions)` # return the number of actions\n",
    "- `print(gridworld.r)` # return a matrix where each element indicates the reward corresponding to each (state, action) pair.\n",
    "- `print(gridworld.gamma)` # return the discount factor\n",
    "- `print(gridworld.sparseT[action])` # Input: action, Return: a matrix containing the state-to-state transition probabilities for the action passed as input.\n",
    "\n",
    "<img src=\"src/vis_gridworld.png\" alt=\"fishy\" class=\"bg-primary\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2f35d",
   "metadata": {},
   "source": [
    "# Ex 1: Prove of the Policy Gradient Theorem via the Performance Difference Lemma (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20dd37",
   "metadata": {},
   "source": [
    "Denote $J(\\pi) = \\langle \\mu, V^\\pi \\rangle$ and recall that the performance difference lemma states\n",
    "$$\n",
    "J(\\pi) - J(\\pi') = \\frac{1}{(1-\\gamma)}\\mathbb{E}_{s \\sim \\lambda^{\\pi'}}[\\langle\\pi(\\cdot|s) - \\pi'(\\cdot| s) , Q^\\pi(s, \\cdot) \\rangle]\n",
    "$$\n",
    "where $\\lambda^{\\pi'} \\in \\Delta_{\\mathcal{S}\\times\\mathcal{A}}$ denotes the occupancy measure of the policy $\\pi'$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a218d",
   "metadata": {},
   "source": [
    "Now let us consider direct parametization, and compute a partial derivative for the entry of $\\pi$ at index $(\\bar{s},\\bar{a})$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9740a2b",
   "metadata": {},
   "source": [
    "**Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb20f5",
   "metadata": {},
   "source": [
    "To help you compute this partial derivative, consider the policies $\\pi'$ parameterized by some (sufficiently small) $\\delta \\in \\mathbb{R}$ via\n",
    "$$\n",
    "    \\pi'(a|s) = \\begin{cases}\n",
    "        \\pi(\\bar{a}|\\bar{s}) + \\delta \\quad (\\text{if } (s,a)=(\\bar{s},\\bar{a}))\\\\\n",
    "        \\pi(a|s) \\quad (\\text{else})\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab6336",
   "metadata": {},
   "source": [
    "(1) Argue that\n",
    "$$\n",
    "\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})} =\\lim_{\\delta \\rightarrow 0} \\frac{\\mathbb{E}_{s \\sim \\lambda^{\\pi'}}[\\langle\\pi(\\cdot|s) - \\pi'(\\cdot| s) , Q^\\pi(s, \\cdot) \\rangle]}{\\pi(\\bar{a}|\\bar{s}) - \\pi'(\\bar{a}|\\bar{s})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81b9f3",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03017f",
   "metadata": {},
   "source": [
    "Once can argue that this is just the definition of the derivative at the point $(\\bar{s},\\bar{a})$ with respect to $\\pi(\\bar{a}|\\bar{s})$. The $J(\\pi) - J(\\pi')$ is the \"$\\Delta y$\" in the slope equation and $\\pi(\\bar{a}|\\bar{s}) - \\pi'(\\bar{a}|\\bar{s})$ is the \"$\\Delta x$\". This gives us the slope of the curve between two points $\\pi$ and $\\pi'$ and then we need to make sure that $\\Delta x$ goes to 0 in order to get the derivative at the point $(\\bar{s},\\bar{a})$ wrt $\\pi(\\bar{a}|\\bar{s})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3ee23",
   "metadata": {},
   "source": [
    "(2) Argue that $$\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})} = \\lim_{\\delta \\rightarrow 0} \\lambda^{\\pi'}(\\bar{s}) Q^\\pi(\\bar{s}, \\bar{a}).$$\n",
    "\n",
    "Hint: Write the expectation in the previous question as a sum and use the fact that $\\frac{\\pi(a|s) - \\pi'(a|s)}{\\pi(\\bar{a}|\\bar{s}) - \\pi'(\\bar{a}|\\bar{s})} = \\mathbf{1}_{\\{ (\\bar{s},\\bar{a}) = (s,a) \\}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39382c3",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b5001",
   "metadata": {},
   "source": [
    "Lets rewrite the expectations on the RHS of the partial derivative in its sum form.\n",
    "$$\n",
    "\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})} = \\lim_{\\delta \\rightarrow 0} \\frac{\\sum_{s}\\lambda^{\\pi'}(s) \\sum_{a}(\\pi(a|s) - \\pi'(a|s)) Q^\\pi(s,a)}{\\pi(\\bar{a}|\\bar{s}) - \\pi'(\\bar{a}|\\bar{s})}\n",
    "$$\n",
    "Now we notice that the sum is only non-zero when $(s, a) = (\\bar{s}, \\bar{a})$. This gives us the following result:\n",
    "$$\n",
    "\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})} = \\lim_{\\delta \\rightarrow 0} \\frac{\\lambda^{\\pi'}(\\bar{s}) [\\pi(\\bar{a}|\\bar{s}) - \\pi'(\\bar{a}|\\bar{s})] Q^\\pi(\\bar{s}, \\bar{a})}{\\pi(\\bar{a}|\\bar{s}) - \\pi'(\\bar{a}|\\bar{s})}\n",
    "$$\n",
    "From which we get our desired result:\n",
    "$$\n",
    "\\boxed{\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})} = \\lim_{\\delta \\rightarrow 0} \\lambda^{\\pi'}(\\bar{s}) Q^\\pi(\\bar{s}, \\bar{a})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f075389",
   "metadata": {},
   "source": [
    "(3) Conclude that $$\\frac{\\partial J(\\pi)}{\\partial \\pi(\\bar{a}|\\bar{s})} = \\lambda^{\\pi}(\\bar{s}) Q^\\pi(\\bar{s}, \\bar{a})$$\n",
    "for the direct parameterization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae1a9d",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92bec4",
   "metadata": {},
   "source": [
    "Since we let delta go to 0, $\\pi'(\\bar{a}|\\bar{s}) = \\pi(\\bar{a}|\\bar{s})$ and thus$\n",
    "$$\n",
    "\\boxed{\\frac{\\partial J(\\pi)}{\\pi(\\bar{a} | \\bar{s})} = \\lim_{\\delta \\rightarrow 0} \\lambda^{\\pi}(\\bar{s}) Q^\\pi(\\bar{s}, \\bar{a}) = \\lambda^{\\pi}(\\bar{s}) Q^\\pi(\\bar{s}, \\bar{a})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343fdf2c",
   "metadata": {},
   "source": [
    "(4) Prove that for a general parametrization, it holds that\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta) = \\sum_{\\bar{s},\\bar{a}} \\lambda^{\\pi}(\\bar{s}, \\bar{a}) Q^\\pi(\\bar{s}, \\bar{a}) \\nabla_{\\theta} ( \\log \\pi_\\theta(\\bar{a}|\\bar{s}))\n",
    "$$\n",
    "\n",
    "Hint: Use the chain rule to write $$ \\nabla_\\theta J(\\pi_\\theta)  = \\sum_{\\bar{s},\\bar{a}} \\frac{\\partial J(\\pi)}{\\partial \\pi_\\theta(\\bar{a}|\\bar{s})} \\nabla_{\\theta} \\pi_\\theta(\\bar{a}|\\bar{s}), $$\n",
    "and then use the fact that $\\lambda^{\\pi}(\\bar{s},\\bar{a}) = \\lambda^{\\pi}(\\bar{s}) \\pi(\\bar{a}|\\bar{s})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c4b89",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8ae8c",
   "metadata": {},
   "source": [
    "From the hint, we use the chain rule to write:\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\pi_{\\theta}) = \\sum_{\\bar{s},\\bar{a}} \\frac{\\partial J(\\pi)}{\\partial \\pi_\\theta(\\bar{a}|\\bar{s})} \\nabla_{\\theta} \\pi_\\theta(\\bar{a}|\\bar{s})\n",
    "$$\n",
    "\n",
    "Now we can substitute the result we derived above to get:\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\pi_{\\theta}) = \\sum_{\\bar{s}, \\bar{a}} \\lambda^{\\pi}(\\bar{s}) Q^\\pi(\\bar{s}, \\bar{a}) \\nabla_{\\theta} \\pi_\\theta(\\bar{a}|\\bar{s})\n",
    "$$\n",
    "\n",
    "Again from the hint we make use of the fact that $\\lambda^{\\pi}(\\bar{s},\\bar{a}) = \\lambda^{\\pi}(\\bar{s}) \\pi(\\bar{a}|\\bar{s})$ to write:\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\pi_{\\theta}) = \\sum_{\\bar{s}, \\bar{a}} \\lambda^{\\pi}(\\bar{s}, \\bar{a}) Q^{\\pi}(\\bar{s}, \\bar{a}) \\frac{\\nabla_{\\theta} \\pi_\\theta(\\bar{a}|\\bar{s})}{\\pi_\\theta(\\bar{a}|\\bar{s})}\n",
    "$$\n",
    "\n",
    "We then make use of the identity that $\\nabla\\log{x} = \\frac{\\nabla x}{x}$ (considering log to be the natual log):\n",
    "$$\n",
    "\\boxed{\\nabla_{\\theta}J(\\pi_{\\theta}) = \\sum_{\\bar{s}, \\bar{a}} \\lambda^{\\pi}(\\bar{s}, \\bar{a}) Q^{\\pi}(\\bar{s}, \\bar{a}) \\nabla_{\\theta} \\log{(\\pi_\\theta(\\bar{a}|\\bar{s}))}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1fc0d1",
   "metadata": {},
   "source": [
    "# Ex 2: Natural Policy Gradient with softmax parameterization (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a963b",
   "metadata": {},
   "source": [
    "Recall that the iterates $\\{\\pi^t\\}^{\\infty}_{t=1}$ produced by NPG read as follows:\n",
    "$$\n",
    "\\pi^{t+1}(a|s) = \\frac{\\pi^t(a|s)e^{\\eta Q^{\\pi^t}(s,a) }}{\\sum_{a'} \\pi^t(a'|s) e^{\\eta Q^{\\pi^t}(s,a')}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ef509",
   "metadata": {},
   "source": [
    "**Question** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b69771",
   "metadata": {},
   "source": [
    "Implement NPG for an arbitrary step size $\\eta$. Please note that $e^{\\eta Q^{\\pi^t}(s,a)}$ can be zero, account for that in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(pi, env, tol=1e-10):\n",
    "    \"\"\"Implementation of policy evaluation through iteratively applying using a certain policy \n",
    "    Args:\n",
    "        pi: a policy stochastic passed with shape n_states times n_actions\n",
    "        env: environment\n",
    "        tol: a scalar to dermerminate whether the policy evaluation convergences\n",
    "    Returns:\n",
    "        v: an array with the values of the actions chosen\n",
    "        q: an array with the q values    \n",
    "    \"\"\"\n",
    "    v = np.zeros(env.n_states)\n",
    "    q = np.zeros((env.n_states, env.n_actions))\n",
    "    while True:\n",
    "        v_old = np.copy(v)\n",
    "        for a in range(env.n_actions):\n",
    "            q[:, a] = env.r[:, a] + env.gamma * env.sparseT[a].dot(v)\n",
    "        for s in range(env.n_states):\n",
    "            v[s] = q[s].dot(pi[s])\n",
    "        if np.linalg.norm(v - v_old) < tol:\n",
    "            break\n",
    "    return v, q\n",
    "\n",
    "def npg_update(q, eta, old_policy):\n",
    "    \"\"\"Implementation of a greedy approach to choose policies (policy improvement)\n",
    "    Args:\n",
    "        q: q values obtained from evaluating the policies\n",
    "    Returns:\n",
    "        new_policy: the updates policy\n",
    "    \"\"\"\n",
    "    policy = np.zeros_like(q)\n",
    "    for s in range(q.shape[0]):\n",
    "        if np.isinf(eta):\n",
    "            policy[s, np.argmax(q[s,:])] = 1\n",
    "        else:\n",
    "            numerator = old_policy[s] * np.exp(eta*q[s])\n",
    "            policy[s] = numerator / (np.sum(numerator) + 1e-10)\n",
    "    return policy\n",
    "\n",
    "def get_greedy_policy(q):\n",
    "    \"\"\"Implementation of a greedy approach to choose policies (policy improvement)\n",
    "    Args:\n",
    "        q: q values obtained from evaluating the policies\n",
    "    Returns:\n",
    "        policy: greedy policy (list)\n",
    "    \"\"\"\n",
    "    policy = np.zeros_like(q)\n",
    "    for s in range(q.shape[0]):\n",
    "        policy[s,np.argmax(q[s,:])] = 1\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003741c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NPG(env, eta): # apply NPG iterations for 30 steps\n",
    "    vs = []\n",
    "    policies = []\n",
    "    v = np.zeros(env.n_states)\n",
    "    q = np.zeros((env.n_states, env.n_actions))\n",
    "    pi = np.ones_like(q)/env.n_actions\n",
    "    for k in range(30):\n",
    "        v_old = np.copy(v)\n",
    "        v, q = evaluate_policy(pi, env)\n",
    "        if eta < np.inf:\n",
    "            pi = npg_update(q, eta, pi)\n",
    "        else:\n",
    "            pi = get_greedy_policy(q)\n",
    "        vs.append(v)\n",
    "        policies.append(pi)\n",
    "    return vs, policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3848dd",
   "metadata": {},
   "source": [
    "Now, we run NPG for different stepsizes in the usual gridworld environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "reward_mode = 2\n",
    "size = 10 \n",
    "prop = 0\n",
    "gamma=0.99\n",
    "gridworld = GridWorldEnvironment(reward_mode, size, prop=0, gamma=gamma)\n",
    "mu = np.ones(gridworld.n_states)/gridworld.n_states\n",
    "etas = [1e-3, 1e-2, 1e-1, 1, 100, 1e7, np.inf]\n",
    "v_different_etas = []\n",
    "pi_different_etas = []\n",
    "for eta in tqdm(etas):\n",
    "    values_pi, policies = NPG(gridworld, eta=eta)\n",
    "    v_different_etas.append(values_pi)\n",
    "    pi_different_etas.append(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34421f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = MDPsolver(gridworld)\n",
    "solver.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: if this plot appears with a too large legend, rerun this line once more\n",
    "plot_log_lines([np.array([mu.dot(solver.v - v) for v in v_different_etas[i]]) for i, _ in enumerate(etas)], [f\"Subopt for eta {eta}\" for eta in etas], [\"Iteration\", \"Subopt\"], \"figs\", \"NPG.pdf\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a32b5",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce2d48",
   "metadata": {},
   "source": [
    "Show that NPG with $\\eta = \\infty$ coincides with Policy Iteration (PI).\n",
    "\n",
    "More formally: Assuming that $a^\\star_s := \\mathrm{argmax}_a Q^{\\pi^t}(s,a)$ is unique for all $s$, prove that $$ \\lim_{\\eta \\rightarrow \\infty} \\frac{\\pi^t(a|s)e^{\\eta Q^{\\pi^t}(s,a) }}{\\sum_{a'} \\pi^t(a'|s) e^{\\eta Q^{\\pi^t}(s,a')}} = \\begin{cases} 1 \\quad \\text{if} \\quad a = a^\\star_s \\\\ 0 \\quad \\text{otherwise} \\end{cases},$$\n",
    "and explain how this relates to PI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8170e",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a7126",
   "metadata": {},
   "source": [
    "Ok so first we fix a state $s \\in \\mathcal{S}$ and define $a^{\\star}_{s} = \\argmax_a Q^{\\pi^t}(s,a)$.\n",
    "Then we can write:\n",
    "$$\n",
    "\\lim_{\\eta \\rightarrow \\infty} \\frac{\\pi^t(a)\\exp{\\eta Q^{\\pi^t}(s,a) }}{\\sum_{a'} \\pi^t(a') \\exp{\\eta Q^{\\pi^t}(s,a')}}\n",
    "$$\n",
    "We then let $M = \\max_{a} Q^{\\pi^t}(s,a)$ and then we have:\n",
    "$$\n",
    "\\lim_{\\eta \\rightarrow \\infty} \\frac{\\pi^t(a)\\exp{\\eta Q^{\\pi^t}(s,a) }}{\\sum_{a'}\\pi^t(a') \\exp{\\eta (Q^{\\pi^t}(s,a') - M)}\\exp(\\eta M)}\n",
    "$$\n",
    "$$\n",
    "\\lim_{\\eta \\rightarrow \\infty} \\frac{\\pi^t(a)\\exp{\\eta (Q^{\\pi^t}(s,a) - M)}}{\\sum_{a'}\\pi^t(a') \\exp{\\eta (Q^{\\pi^t}(s,a') - M)}}\n",
    "$$\n",
    "\n",
    "Then we can see that:\n",
    " - $Q(s, a) - M < 0 \\text{ if } a \\neq a^{\\star}$\n",
    " - $Q(s, a) - M = 0 \\text{ if } a = a^{\\star}$\n",
    "\n",
    "We can then split up the sum in the denominator:\n",
    "$$\n",
    "\\lim_{\\eta \\rightarrow \\infty} \\frac{\\pi^t(a)\\exp{\\eta (Q^{\\pi^t}(s,a) - M)}}{\\pi^t(a^{\\star})\\exp{0} + \\sum_{a' \\neq a^{\\star}}\\pi^t(a') \\exp{\\eta (Q^{\\pi^t}(s,a') - M)}}\n",
    "$$\n",
    "\n",
    "Now the second term in the denominator goes to 0 as $\\eta \\rightarrow \\infty$. In the numerator we only get a non-zero answer if $a = a^{\\star}$, in that case the answer is 1. Gathering this information together we can see that this is the same as PI and we arrive at the following definition:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "    \\begin{align*}\n",
    "        \\lim_{\\eta \\rightarrow \\infty} \\frac{\\pi^t(a)\\exp{\\eta (Q^{\\pi^t}(s,a) - M)}}{\\pi^t(a^{\\star})\\exp{0} + \\sum_{a' \\neq a^{\\star}}\\pi^t(a') \\exp{\\eta (Q^{\\pi^t}(s,a') - M)}} &= \\begin{cases} 1 \\quad \\text{if} \\quad a = a^{\\star} \\\\ 0 \\quad \\text{otherwise} \\end{cases} \\\\\n",
    "        \\Leftrightarrow \\lim_{\\eta \\rightarrow \\infty} \\frac{\\pi^t(a|s)\\exp{{\\eta Q^{\\pi^t}(s,a)} }}{\\sum_{a'} \\pi^t(a'|s) \\exp{\\eta Q^{\\pi^t}(s,a')}} &= \\begin{cases} 1 \\quad \\text{if} \\quad a = a^\\star_s \\\\ 0 \\quad \\text{otherwise} \\end{cases}\n",
    "    \\end{align*}\n",
    "}\n",
    "$$\n",
    "\n",
    "Which is equivalent to what question asked us to prove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d927c2",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74dd8a",
   "metadata": {},
   "source": [
    "Is this observation in line with the empirical results in the plot above? I.e., is the plot for $\\eta = \\infty$ as you would expect it for PI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a00c6",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ee997",
   "metadata": {},
   "source": [
    "Yes this plot is similar to the plots we saw in Homework 1 when working with the PI algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc1863",
   "metadata": {},
   "source": [
    "# Ex 2.1 Slow Changing Property of NPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13376b5f",
   "metadata": {},
   "source": [
    "In this exercise you will investigate by how much consecutive iterates $\\pi^t$ and $\\pi^{t+1}$ produced by NPG differ and how this distance is controlled by the step size $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd4b77",
   "metadata": {},
   "source": [
    "Plot $$\\max_{s \\in \\mathcal{S}} || \\pi^{t+1}(a|s) - \\pi^t(a|s) ||_1$$ for different values of $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_variation(policies):\n",
    "    variation = []\n",
    "    for pi, pip in zip(policies[1:], policies[:-1]):\n",
    "        variation.append(np.max([ np.linalg.norm(pi[s] - pip[s], 1) for s in range(pi.shape[0])]))\n",
    "    return variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines(np.array([ compute_policy_variation(np.array(pi_different_etas)[i])\n",
    "                           for i, _ in enumerate(etas)]), \n",
    "               [f\" eta = {eta}\" for eta in etas], \n",
    "               [\"Iteration\", \"Variation\"], \"figs\", \"NPG.pdf\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0b5d5",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22525a76",
   "metadata": {},
   "source": [
    "Empirically, is the largest change (among all iterations) between consecutive iterations is larger for smaller or large values of $\\eta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e839b",
   "metadata": {},
   "source": [
    "It is smaller for larger values of $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62fa81b",
   "metadata": {},
   "source": [
    "## Some Theory to Motivate the Observation Above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3cf2d",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Our goal is to prove that $$ || \\pi^{t+1}(\\cdot|s) - \\pi^t(\\cdot|s) ||_1 \\leq \\frac{\\eta}{1 - \\gamma} \\quad \\forall s \\in \\mathcal{S}, \\forall t \\in [T].$$\n",
    "\n",
    "We guide you towards this result by breaking the proof into small steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24f01a",
   "metadata": {},
   "source": [
    "1) Prove that $$ \\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 \\leq \\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[\\eta Q^{\\pi^t}(s,a)] - \\log \\bigg(\\sum_{a'\\in\\mathcal{A}} \\pi^t(a'|s) \\exp (\\eta Q^{\\pi^t}(s,a'))\\bigg) $$\n",
    "\n",
    "Hint: First apply Pinkser's inequality https://en.wikipedia.org/wiki/Pinsker%27s_inequality to prove that $$\\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 \\leq KL(\\pi^{t+1}(s)||\\pi^t(s)), $$ then plug in the formula for $\\pi^{t+1}$ into the KL term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d76b91",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61921e0e",
   "metadata": {},
   "source": [
    "Using the hint provided we first start off by using Pinsker's inequality to get:\n",
    "\n",
    "$$\n",
    "\\lVert \\pi^{t+1}(s) - \\pi^t(s) \\rVert_1 \\leq \\sqrt{2\\operatorname{KL}(\\pi^{t+1}(s) || \\pi^{t}(s))}\n",
    "$$\n",
    "\n",
    "By squaring both sides and rearranging the terms we get:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\\lVert \\pi^{t+1}(s) - \\pi^{t}(s) \\rVert_1^2 \\leq \\operatorname{KL}(\\pi^{t+1}(s) || \\pi^{t}(s))\n",
    "$$\n",
    "\n",
    "We can now substitute terms into the KL divergence to get:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{1}{2}\\lVert \\pi^{t+1}(s) - \\pi^{t}(s) \\rVert_1^2 &\\leq \\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}\\left[\\log\\left(\\frac{\\exp(\\eta Q^{\\pi^{t}(s, a)})}{\\sum_{a'\\in \\mathcal{A}}\\pi^{t}(s, a') \\exp(\\eta Q^{\\pi^{t}(s, a')})}\\right)\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "Since the denominator is a constant wrt to the expection we can pull it out of the expectation and arrive at out final answer:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{1}{2}\\lVert \\pi^{t+1}(s) - \\pi^{t}(s) \\rVert_1^2 \\leq \\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[\\eta Q^{\\pi^t}(s,a)] - \\log \\bigg(\\sum_{a'\\in\\mathcal{A}} \\pi^t(a'|s) \\exp (\\eta Q^{\\pi^t}(s,a'))\\bigg).\n",
    "}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b4a0e",
   "metadata": {},
   "source": [
    "2) Prove that \n",
    "$$\n",
    "\\sum_{a\\in \\mathcal{A}} \\pi^{t+1}(a|s) \\exp(- \\eta Q^{\\pi^t}(s,a)) = \\frac{1}{\\sum_{a'\\in \\mathcal{A}} \\pi^t(a|s) \\exp(\\eta Q^{\\pi^t}(s,a) )}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be47fbb",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56f638",
   "metadata": {},
   "source": [
    "To prove the equation above we can substitute in the definition of $\\pi^{t+1}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{a \\in \\mathcal{A}} \\pi^{t+1}(a | s)\\exp(-\\eta Q^{\\pi^t}(s, a)) &= \\sum_{a \\in \\mathcal{A}} \\frac{\\pi^t(a | s) \\exp(\\eta Q^{\\pi^t}(s, a)) \\exp(-\\eta Q^{\\pi^t}(s, a)}{\\sum_{a' \\in \\mathcal{A}} \\pi^t(a' | s) \\exp(\\eta Q^{\\pi^t}(s, a'))} \\\\\n",
    "&= \\frac{1}{\\sum_{a' \\in \\mathcal{A}} \\pi^t(a' | s) \\exp(\\eta Q^{\\pi^t}(s, a'))}\\left[\\sum_{a \\in \\mathcal{A}} \\pi^t(a | s)\\right] \\\\\n",
    "\n",
    "&\\boxed{\n",
    "    = \\frac{1}{\\sum_{a' \\in \\mathcal{A}} \\pi^t(a' | s) \\exp(\\eta Q^{\\pi^t}(s, a'))} \n",
    "}\n",
    "\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fa85b",
   "metadata": {},
   "source": [
    "3) Using the results in 1) and 2) prove that \n",
    "\n",
    "$$ \\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 \\leq \\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[\\eta Q^{\\pi^t}(s,a)] + \\log \\bigg(\\sum_{a'\\in\\mathcal{A}} \\pi^{t+1}(a'|s) \\exp (-\\eta Q^{\\pi^t}(s,a'))\\bigg). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f5ea0",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b902403f",
   "metadata": {},
   "source": [
    "We begin by rewriting our results from 1):\n",
    "$$\n",
    "\\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[\\eta Q^{\\pi^t}(s,a)] - \\log \\bigg(\\sum_{a'\\in\\mathcal{A}} \\pi^t(a'|s) \\exp (\\eta Q^{\\pi^t}(s,a'))\\bigg) = \\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[\\eta Q^{\\pi^t}(s,a)] + \\log \\frac{1}{(\\sum_{a'\\in\\mathcal{A}} \\pi^t(a'|s) \\exp (\\eta Q^{\\pi^t}(s,a')))}\n",
    "$$\n",
    "\n",
    "Then we can substitute out result from 2) into the log term to get the inequaltiy:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "    \\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 \\leq \\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[\\eta Q^{\\pi^t}(s,a)] + \\log \\bigg(\\sum_{a'\\in\\mathcal{A}} \\pi^{t+1}(a'|s) \\exp (-\\eta Q^{\\pi^t}(s,a'))\\bigg)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f7f1d",
   "metadata": {},
   "source": [
    "4) Using Hoeffding's Lemma https://en.wikipedia.org/wiki/Hoeffding%27s_lemma (on the sum in the log term!) and the fact that $$-\\frac{1}{1-\\gamma} \\leq Q^{\\pi^t}(s,a) \\leq \\frac{1}{1-\\gamma},$$ conclude that \n",
    "$$\\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 \\leq \\frac{\\eta^2}{2 (1 - \\gamma)^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4459f5",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "We can rewrite the sum inside the log as an expectation:\n",
    "\n",
    "$$\n",
    "\\sum_{a' \\in \\mathcal{A}} \\pi^{t+1}(a'|s) \\exp (-\\eta Q^{\\pi^t}(s,a')) = \\mathbb{E}_{a' \\sim \\pi^{t+1}(\\cdot|s)}[\\exp (-\\eta Q^{\\pi^t}(s,a'))]\n",
    "$$\n",
    "\n",
    "This then allows us to use Hoeffding's Lemma (as the question suggested) to get:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[\\eta Q^{\\pi^t}(s,a)] &\\leq \\exp\\left(-\\eta\\mathbb{E}_{a' \\sim \\pi^{t+1}(\\cdot|s)}[Q^{\\pi^t}(s,a')] + \\frac{\\eta^{2}(\\frac{1}{1-\\gamma} + \\frac{1}{1-\\gamma})^2}{8} \\right)\\\\\n",
    "\n",
    "&= \\exp\\left(-\\eta\\mathbb{E}_{a' \\sim \\pi^{t+1}(\\cdot|s)}[Q^{\\pi^t}(s,a')] + \\frac{\\eta^{2}(\\frac{2}{1-\\gamma})^2}{8} \\right) \\\\\n",
    "\n",
    "&= \\exp\\left(-\\eta\\mathbb{E}_{a' \\sim \\pi^{t+1}(\\cdot|s)}[Q^{\\pi^t}(s,a')] + \\frac{\\eta^2}{2(1-\\gamma)^2} \\right) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now we can substitute this back into the original equation to get:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 &\\leq \\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[\\eta Q^{\\pi^t}(s,a)] + \\log \\bigg(\\sum_{a'\\in\\mathcal{A}} \\pi^{t+1}(a'|s) \\exp (-\\eta Q^{\\pi^t}(s,a'))\\bigg) \\\\\n",
    "&\\leq \\eta\\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[Q^{\\pi^t}(s,a)] + \\log\\left(\\exp\\left(-\\eta\\mathbb{E}_{a' \\sim \\pi^{t+1}(\\cdot|s)}[Q^{\\pi^t}(s,a')] + \\frac{\\eta^2}{2(1-\\gamma)^2} \\right)\\right) \\\\\n",
    "&= \\eta\\mathbb{E}_{a \\sim \\pi^{t+1}(\\cdot|s)}[Q^{\\pi^t}(s,a)] -\\eta\\mathbb{E}_{a' \\sim \\pi^{t+1}(\\cdot|s)}[Q^{\\pi^t}(s,a')] + \\frac{\\eta^2}{2(1-\\gamma)^2} \\\\\n",
    "&= \\frac{\\eta^2}{2(1-\\gamma)^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And so we have shown that:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "    \\frac{1}{2} || \\pi^{t+1}(s) - \\pi^t(s) ||^2_1 \\leq \\frac{\\eta^2}{2 (1 - \\gamma)^2}\n",
    "}\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "To finish the proof we can multiply by 2 and take the square root to get:\n",
    "$$\n",
    "\\boxed{\n",
    "|| \\pi^{t+1}(\\cdot|s) - \\pi^t(\\cdot|s) ||_1 \\leq \\frac{\\eta}{1 - \\gamma} \\quad \\forall s \\in \\mathcal{S}, \\forall t \\in [T]\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2621f87",
   "metadata": {},
   "source": [
    "# Ex 3: OPPO: The importance of Exploration in Policy Gradient (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86788fb",
   "metadata": {},
   "source": [
    "In this exercise, we will investigate how crucial it is to perform exploration. That is, adding bonuses to avoid suffering the mismatch coefficients in the convergence bounds.\n",
    "\n",
    "Let us recall that the standard sample based version of NPG suffers the mismatch coeffcients in the bounds (see Slide 22 in Lecture 5). Those are avoided by OPPO ( See slide 30 in Lecture 5 ).\n",
    "\n",
    "**To see clearly the advatange of OPPO we will consider an MDP with unbounded mismatch coefficients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032cbad",
   "metadata": {},
   "source": [
    "**Question: example of unbounded mismatch coefficients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df78d50",
   "metadata": {},
   "source": [
    "Consider a 10 x 10 gridworld, the initial state is always the bottom right corner, i.e. the initial distribution $\\mu$ equals $1$ at this starting state and it is zero everywhere else. Can you compute a finite bound for \n",
    "$$\\max_\\pi \\max_{s \\in \\mathcal{S}} \\bigg |\\frac{\\lambda^\\pi(s)}{\\mu(s)} \\bigg|,$$\n",
    "i.e. the mismatch coefficient? If not, argue for which reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fa3b9",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a194249",
   "metadata": {},
   "source": [
    "No it is not possible to compute a finite bound for this. We will show this in the following calculations. Lets define $s_0 \\in \\mathcal{S}$ to be the initial state. By definition $\\mu{s_0} = 1$ and $\\mu{s} = 0$ for all $s \\in \\mathcal{S} \\setminus \\{s_0\\}$ So we have:\n",
    "\n",
    "$$\n",
    "\\left\\lvert \\frac{\\lambda^{\\pi}(s)}{\\mu(s)} \\right\\rvert = \\begin{cases} \\lambda^{\\pi}(s) \\text{ if } s = s_0 \\\\ 0 \\text{ otherwise } \\infty \\end{cases}\n",
    "$$\n",
    "\n",
    "Since we are maximising over all states and all possible policies, there could exist a policy that moves us away from $s_0$ and then the maximum would be $\\infty$. So we cannot compute a finite bound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607852c8",
   "metadata": {},
   "source": [
    "In the following, we experiment with OPPO with and without bonuses in this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mode = 0\n",
    "size = 10\n",
    "gamma=0.999\n",
    "gridworld = GridWorldEnvironment(reward_mode, size, prop=0, gamma=gamma)\n",
    "r_max = np.max(gridworld.r)\n",
    "r_min = np.min(gridworld.r)\n",
    "gridworld.r = (gridworld.r - r_min) / (r_max - r_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "def oppo(K: int = 10000, H: int = 20, beta: float = 0.0001, eta=10000) -> List[float]:\n",
    "    \"\"\"\n",
    "    Function implementing OPPO with UCB bonuses algorithm.\n",
    "\n",
    "    :param K: Number of episodes, positive int\n",
    "    :param H: Number of steps per episode, positive int\n",
    "    :param beta: Algorithm hyperparameter, constant which scales the bonuses, positive float\n",
    "\n",
    "    :return: reward after each step, list of K * H floats\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize tabular records\n",
    "    rewards = []\n",
    "    Q = H * np.ones((H, gridworld.n_states, gridworld.n_actions))\n",
    "    V = H * np.ones((H + 1, gridworld.n_states))\n",
    "    policy = H * np.ones((H, gridworld.n_states, gridworld.n_actions))/gridworld.n_actions\n",
    "    V[H, :] = 0\n",
    "    estimated_transitions = np.ones((H, gridworld.n_states, \n",
    "                                     gridworld.n_actions, \n",
    "                                     gridworld.n_states))/gridworld.n_states\n",
    "    N = np.zeros((H, gridworld.n_states, gridworld.n_actions))\n",
    "    bonus = np.zeros((H, gridworld.n_states, gridworld.n_actions))\n",
    "    N_next = np.zeros((H, gridworld.n_states, gridworld.n_actions, gridworld.n_states))\n",
    "\n",
    "    for k in tqdm(range(K), total=K, desc=\"Running an Episode...\", leave=False):  # Episode loop\n",
    "        #print(k, \"k\")\n",
    "        state = 99  # Initial state\n",
    "        for h in tqdm(range(H), total=H, desc=\"Running a Horizon Step...\", leave=False):  # Step loop\n",
    "            #NPG Update\n",
    "            advantage = eta * (Q[h, state] - np.max(Q[h, state]))  # numerical stability\n",
    "            unnormalized = policy[h, state] * np.exp(advantage)\n",
    "            policy[h, state] = unnormalized / (np.sum(unnormalized) + 1e-10)\n",
    "            \n",
    "            # Sample one action the current policy\n",
    "            a = np.random.choice(gridworld.n_actions, p=policy[h, state])\n",
    "            rewards.append(gridworld.r[state, a])\n",
    "\n",
    "            # Record that we visited this state-action pair (again)\n",
    "            N[h, state, a] += 1\n",
    "\n",
    "            # Get the new state according to the transition dynamics\n",
    "            new_state = np.random.choice(gridworld.n_states,\n",
    "                                         p=gridworld.T[a][state])\n",
    "            N_next[h, state, a, new_state] += 1\n",
    "            \n",
    "            estimated_transitions[h,state,a,:] = N_next[h, state, a, new_state] / (N[h, state, a] + 1)\n",
    "            \n",
    "\n",
    "            # Calculate the UCB bonus\n",
    "            state = new_state\n",
    "        \n",
    "        bonus = np.sqrt(beta/(N+1e-10)) #Should be of dimension H x Sx A\n",
    "        Q_new = np.zeros_like(Q)\n",
    "        V_new = np.zeros_like(V)\n",
    "        for h in tqdm(reversed(range(H)), total=H, desc=\"Performing backward horizon step\", leave=False):  # Step loop\n",
    "            # Update Q according to the algorithm\n",
    "            expected_Q = np.einsum(\n",
    "                'sat,tb->sa',\n",
    "                estimated_transitions[h],\n",
    "                policy[h + 1] * Q[h + 1] if h + 1 < H else np.zeros_like(Q[0])\n",
    "            )\n",
    "\n",
    "            Q_new[h] = gridworld.r + bonus[h] + expected_Q\n",
    "            #This clipping is to ensure that the bonus and estimated transitions cant give\n",
    "            # you a value of Q that is higher than you could possibly get.\n",
    "            Q_new[h] = np.clip(Q_new[h], 0, H-h+1)\n",
    "\n",
    "            # Update V as the Q-value of the optimal actions for the current state\n",
    "            V_new[h] = np.sum(policy[h] * Q_new[h], axis=1)\n",
    "        #Q = deepcopy(Q_new)\n",
    "        #V = deepcopy(V_new)\n",
    "        Q[...] = Q_new\n",
    "        V[...] = V_new\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340404be",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = []\n",
    "betas = [0,1e-5, 1e-4, 1e-3, 1e-1]\n",
    "\n",
    "for beta in tqdm(betas, total=len(betas), desc=\"Iterating through beta values...\"):\n",
    "    reward_OPPO = oppo(beta = beta)  # You can play around with the arguments if you like\n",
    "    to_plot.append(np.cumsum(reward_OPPO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41fce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ f\"OPPO beta = {beta}\" for beta in betas]\n",
    "plot_lines(\n",
    "    to_plot,\n",
    "    labels,\n",
    "    [\"Iteration\", \"Reward collected so far\"],\n",
    "    \"figs\",\n",
    "    \"ucbvseps\",\n",
    "    show=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c551415",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f8834",
   "metadata": {},
   "source": [
    "Why does setting $\\beta = 0$ lead to bad results? \n",
    "\n",
    "*Hint: Explain using the remarks in slide 28 and the theoretical bound in Slide 22 of Lecture 5*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c806d",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469ffed",
   "metadata": {},
   "source": [
    "The $\\beta$ paramter in OPPO is used when we try to compute the bonus. This also means that we dont effectively explore the space and our updates to Q are too pessimistic leading to a large value of $\\kappa$ and a high statistical error $\\epsilon_{stat}$. Also setting $\\beta= 0$ means that optimism does not hold since the $Q_h^{t}$ that we estimate will be pesimistic ie:\n",
    "\n",
    "$$\n",
    "Q_{h,\\beta}^{t}(s, a) \\leq r_h^{t}(s, a) + \\sum_{s'} \\hat{P}^{t}(s'| s, a) V^{t}_h(s')\n",
    "$$\n",
    "\n",
    "Meaning that $Q^{\\pi_{t}}(s, a)$ will not be a fixed point of this.\n",
    "\n",
    "If our bonus is too small it could also mean that bounded optimism will not hold anymore "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468763e4",
   "metadata": {},
   "source": [
    "**Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acaeed3",
   "metadata": {},
   "source": [
    "Why does setting $\\beta$ too large lead to poor results?\n",
    "\n",
    "*Hint: Answer using the regret bound for OPPO given at the beginning of slide 30.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8018b",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125a7fc",
   "metadata": {},
   "source": [
    "If the beta is too large then the bonus will be too large and the value of Q will be too optimistic. This also means that our regret bound will be very large. From slide 30 we get the following regret bound:\n",
    "\n",
    "$$\n",
    "\\sum^{T}_{t=1} V^{\\star}(s_1) - V^{\\pi_t}(s_1) \\leq \\mathcal{O}\\left(\\sum^T_{t=1} \\sum^H_{h=1} \\mathrm{bonus}(s^t_h, a^t_h)\\right)\n",
    "$$\n",
    "\n",
    "Since the difference between our iterates and the optimial depend on the bonus, if we overestimate it by using a large $\\beta$ then our difference will be very large and affect our convergence rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ab81f",
   "metadata": {},
   "source": [
    "# Ex 4: REINFORCE with parametrized policies (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RJjcHYbq4u0b",
   "metadata": {
    "id": "RJjcHYbq4u0b"
   },
   "source": [
    "In this exercise, we will investigate the effect of choosing different baselines in the reinforce implementation.\n",
    "This topic is covered from Slide 31 on in Lecture 5.\n",
    "\n",
    "**Hint: You may want to use Google Colab to run the experiments faster, but you don't have to.**\n",
    "\n",
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8691d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym==0.25.2\n",
      "  Downloading gym-0.25.2.tar.gz (734 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.5/734.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /home/ubuntu/miniconda3/envs/RL/lib/python3.11/site-packages (from gym==0.25.2) (1.26.4)\n",
      "Collecting cloudpickle>=1.2.0 (from gym==0.25.2)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gym_notices>=0.0.4 (from gym==0.25.2)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.25.2-py3-none-any.whl size=852398 sha256=473d80f553216817f1c50207a9027701a06899234cfdfa46922a18b8f2d38e74\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/7e/09/48/68ead2493e0b2c26cb6aeb5a712d379f638b5cc21d3501e2f0\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.1.1 gym-0.25.2 gym_notices-0.0.8\n",
      "Requirement already satisfied: gym-notices==0.0.8 in /home/ubuntu/miniconda3/envs/RL/lib/python3.11/site-packages (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "# TODO: you may need to run this to make sure to have the correct versions\n",
    "!pip install gym==0.25.2\n",
    "!pip install gym-notices==0.0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ALUGQk5x4u0b",
   "metadata": {
    "executionInfo": {
     "elapsed": 6543,
     "status": "ok",
     "timestamp": 1710702571987,
     "user": {
      "displayName": "Adrian Müller",
      "userId": "01347970328935212551"
     },
     "user_tz": -60
    },
    "id": "ALUGQk5x4u0b"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import base64, io\n",
    "\n",
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "yCUCh_HY4u0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1710702571987,
     "user": {
      "displayName": "Adrian Müller",
      "userId": "01347970328935212551"
     },
     "user_tz": -60
    },
    "id": "yCUCh_HY4u0d",
    "outputId": "2de5cdc9-50e3-4bc8-ff81-4e7ce808f69d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5H7mDuS4u0e",
   "metadata": {
    "id": "e5H7mDuS4u0e"
   },
   "source": [
    "### Instantiate the Environment and Agent\n",
    "\n",
    "The CartPole environment is very simple. It has discrete action space (2) and 4 dimensional state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "SSoX_CHo4u0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1710702589197,
     "user": {
      "displayName": "Adrian Müller",
      "userId": "01347970328935212551"
     },
     "user_tz": -60
    },
    "id": "SSoX_CHo4u0e",
    "outputId": "2c2d3f67-949d-4950-b098-4ff6ed9eff59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/RL/lib/python3.11/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/ubuntu/miniconda3/envs/RL/lib/python3.11/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/ubuntu/miniconda3/envs/RL/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/ubuntu/miniconda3/envs/RL/lib/python3.11/site-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a1e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module): # definie the policy network\n",
    "    def __init__(self, state_size=4, action_size=2, hidden_size=32):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1) # we just consider 1 dimensional probability of action\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf43e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE (with reward-to-go)\n",
    "# --> with gradient estimator according to version 2 of the PG theorem (not using Q-values, but reward to go)\n",
    "def reinforce_rwd2go(policy, optimizer, early_stop=False, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        # Calculate total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # Recalculate the total reward applying discounted factor\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        rewards_to_go = [sum([discounts[j]*rewards[j+t] for j in range(len(rewards)-t) ]) for t in range(len(rewards))]\n",
    "\n",
    "        # Calculate the loss\n",
    "        policy_loss = []\n",
    "        for i in range(len(saved_log_probs)):\n",
    "            log_prob = saved_log_probs[i]\n",
    "            G = rewards_to_go[i]\n",
    "            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        # After that, we concatenate whole policy loss in 0th dimension\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "        if early_stop and np.mean(scores_deque) >= 195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9338c",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "1. Find **two** good baselines that improve over the implementation of REINFORCE without baseline. You should plot their results below.\n",
    "\n",
    "You can take inspiration from the Example Notebook we attached for lecture 4, but you **cannot use exactly the same**.\n",
    "\n",
    "2. Explain why you chose your baselines and why you think they are reasonable.\n",
    "\n",
    "*Note:* You may also change other parameters such as the learning rate, as long as you clearly state it in your response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882949ca",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b1ab9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb926e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_baseline(state): # Example Baseline from lecture 4 (for inspiration)\n",
    "  angle = state[2]\n",
    "  value = 100*(0.25-angle**2) # TO BE CHANGED USING YOUR BASELINE\n",
    "  return value\n",
    "\n",
    "def baseline_1(state): # TO BE CHANGED USING YOUR BASELINE 1\n",
    "  ???   # TODO\n",
    "  return ??? # TODO\n",
    "\n",
    "def baseline_2(state): # TO BE CHANGED USING YOUR BASELINE 2\n",
    "  ???   # TODO\n",
    "  return ??? # TODO\n",
    "\n",
    "# PLOT 3: reward-to-go with baseline REINFORCE\n",
    "# --> with gradient estimator according to version 3 of the PG theorem (not using Q-values, but reward to go)\n",
    "# --> here, we consider only fixed (handcrafted) baseline functions b : S -> R; clearly, training a NN to predict V^{\\pi}(s) as a baseline is also possible (and interesting!)\n",
    "def reinforce_rwd2go_baseline(policy, optimizer, early_stop=False, baseline=naive_baseline, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        baseline_values = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            baseline_values.append(baseline(state))\n",
    "            if done:\n",
    "                break\n",
    "        # Calculate total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # Recalculate the total reward applying discounted factor\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        rewards_to_go = [sum([discounts[j]*rewards[j+t] for j in range(len(rewards)-t) ]) for t in range(len(rewards))]\n",
    "\n",
    "        # Calculate the loss\n",
    "        policy_loss = []\n",
    "        for i in range(len(saved_log_probs)):\n",
    "            log_prob = saved_log_probs[i]\n",
    "            G_centered = rewards_to_go[i] - baseline_values[i]\n",
    "            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.\n",
    "            policy_loss.append(-log_prob * G_centered)\n",
    "        # After that, we concatenate whole policy loss in 0th dimension\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "        if early_stop and np.mean(scores_deque) >= 195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1619add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "# PLOT 1: run REINFORCE\n",
    "policy_rwd2go = Policy().to(device)\n",
    "optimizer_rwd2go = optim.Adam(policy_rwd2go.parameters(), lr=1e-2)\n",
    "scores_rwd2go = reinforce_rwd2go(policy_rwd2go, optimizer_rwd2go, early_stop=False, n_episodes=2000)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "# PLOT 2: run REINFORCE and YOUR baseline 1\n",
    "policy_baseline_1 = Policy().to(device)\n",
    "optimizer_baseline_1 = optim.Adam(policy_baseline_1.parameters(), lr=1e-2)\n",
    "scores_baseline_1 = reinforce_rwd2go_baseline(policy_baseline_1, optimizer_baseline_1, baseline=baseline_1, early_stop=False, n_episodes=2000)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "# PLOT 3: run REINFORCE and YOUR baseline 2\n",
    "policy_baseline_2 = Policy().to(device)\n",
    "optimizer_baseline_2 = optim.Adam(policy_baseline_2.parameters(), lr=1e-2)\n",
    "scores_baseline_2 = reinforce_rwd2go_baseline(policy_baseline_2, optimizer_baseline_2, baseline=baseline_2, early_stop=False, n_episodes=2000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the learning progress\n",
    "\n",
    "# Create the plot\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot the scores with specified colors and labels\n",
    "ax.plot(np.arange(1, len(scores_rwd2go) + 1), scores_rwd2go, color='green', label='No Baseline')\n",
    "ax.plot(np.arange(1, len(scores_baseline_1) + 1), scores_baseline_1, color='blue', label='Baseline 1')\n",
    "ax.plot(np.arange(1, len(scores_baseline_2) + 1), scores_baseline_2, color='red', label='Baseline 2')\n",
    "\n",
    "# Set the labels with a larger font size\n",
    "ax.set_ylabel('Total reward (= time balanced)', fontsize=20)\n",
    "ax.set_xlabel('Episode #', fontsize=20)\n",
    "\n",
    "# Set the tick labels to a larger font size\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "# Add a legend with a specified font size\n",
    "ax.legend(fontsize=20)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66ac34",
   "metadata": {},
   "source": [
    "# $Q^\\star$: Policy Gradient with continuous actions and bound on the bonuses count in OPPO (20 points)\n",
    "***Question 1:*** Consider using a Gaussian parameterized policy $\\pi_{\\mu,\\Sigma}$ with mean $\\mu \\in \\mathrm{R}^d$ \n",
    "and covariance matrix $\\Sigma$ . Write down the following gradients:\n",
    "\n",
    "$$ \\nabla_\\mu J(\\pi_{\\mu, \\Sigma}) = ???$$\n",
    "$$ \\nabla_\\Sigma J(\\pi_{\\mu, \\Sigma}) = ???$$\n",
    "***Question 2*** In this exercise, you will bound the state action counts. This is a crucial part of the OPPO convergence proof. Let $N^t_h(s,a)$ denotes the number of times the state action pair $s,a$ has been visited at step $h$ in all the episode up to $t$ included. Moreover,\n",
    "let $s^t_h,a^t_h$ be the state action pair visited at step $h$ of the $t^{th}$ episode. Then, prove that \n",
    "$$ \\sum^T_{t=1} \\sum^H_{h=1} \\frac{1}{N^t_h(s^t_h, a^t_h)+1} \\leq SA \\log( T H)$$\n",
    "***Question 3*** Use the fact above to prove the following bound at slide 30 of Lecture 5. That is, for $\\mathrm{bonus}^t_h(s,a) = \\frac{H}{\\sqrt{N^t_h(s,a)}}$ it holds that\n",
    "$$ \\sum^T_{t=1} \\sum^H_{h=1} \\mathrm{bonus}(s^t_h, a^t_h) \\leq \\sqrt{ H^3 SA T \\log( T H)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc2e77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
